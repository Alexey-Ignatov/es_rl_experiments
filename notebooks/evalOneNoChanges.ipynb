{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf # pylint: ignore-module\n",
    "import builtins\n",
    "import functools\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# ================================================================\n",
    "# Import all names into common namespace\n",
    "# ================================================================\n",
    "\n",
    "clip = tf.clip_by_value\n",
    "\n",
    "# Make consistent with numpy\n",
    "# ----------------------------------------\n",
    "\n",
    "def sum(x, axis=None, keepdims=False):\n",
    "    return tf.reduce_sum(x, reduction_indices=None if axis is None else [axis], keep_dims = keepdims)\n",
    "def mean(x, axis=None, keepdims=False):\n",
    "    return tf.reduce_mean(x, reduction_indices=None if axis is None else [axis], keep_dims = keepdims)\n",
    "def var(x, axis=None, keepdims=False):\n",
    "    meanx = mean(x, axis=axis, keepdims=keepdims)\n",
    "    return mean(tf.square(x - meanx), axis=axis, keepdims=keepdims)\n",
    "def std(x, axis=None, keepdims=False):\n",
    "    return tf.sqrt(var(x, axis=axis, keepdims=keepdims))\n",
    "def max(x, axis=None, keepdims=False):\n",
    "    return tf.reduce_max(x, reduction_indices=None if axis is None else [axis], keep_dims = keepdims)\n",
    "def min(x, axis=None, keepdims=False):\n",
    "    return tf.reduce_min(x, reduction_indices=None if axis is None else [axis], keep_dims = keepdims)\n",
    "def concatenate(arrs, axis=0):\n",
    "    return tf.concat(axis, arrs)\n",
    "def argmax(x, axis=None):\n",
    "    return tf.argmax(x, dimension=axis)\n",
    "\n",
    "def switch(condition, then_expression, else_expression):\n",
    "    '''Switches between two operations depending on a scalar value (int or bool).\n",
    "    Note that both `then_expression` and `else_expression`\n",
    "    should be symbolic tensors of the *same shape*.\n",
    "\n",
    "    # Arguments\n",
    "        condition: scalar tensor.\n",
    "        then_expression: TensorFlow operation.\n",
    "        else_expression: TensorFlow operation.\n",
    "    '''\n",
    "    x_shape = copy.copy(then_expression.get_shape())\n",
    "    x = tf.cond(tf.cast(condition, 'bool'),\n",
    "                lambda: then_expression,\n",
    "                lambda: else_expression)\n",
    "    x.set_shape(x_shape)\n",
    "    return x\n",
    "\n",
    "# Extras\n",
    "# ----------------------------------------\n",
    "def l2loss(params):\n",
    "    if len(params) == 0:\n",
    "        return tf.constant(0.0)\n",
    "    else:\n",
    "        return tf.add_n([sum(tf.square(p)) for p in params])\n",
    "def lrelu(x, leak=0.2):\n",
    "    f1 = 0.5 * (1 + leak)\n",
    "    f2 = 0.5 * (1 - leak)\n",
    "    return f1 * x + f2 * abs(x)\n",
    "def categorical_sample_logits(X):\n",
    "    # https://github.com/tensorflow/tensorflow/issues/456\n",
    "    U = tf.random_uniform(tf.shape(X))\n",
    "    return argmax(X - tf.log(-tf.log(U)), axis=1)\n",
    "\n",
    "# ================================================================\n",
    "# Global session\n",
    "# ================================================================\n",
    "\n",
    "def get_session():\n",
    "    return tf.get_default_session()\n",
    "\n",
    "def single_threaded_session():\n",
    "    tf_config = tf.ConfigProto(\n",
    "        inter_op_parallelism_threads=1,\n",
    "        intra_op_parallelism_threads=1)\n",
    "    return tf.Session(config=tf_config)\n",
    "\n",
    "ALREADY_INITIALIZED = set()\n",
    "def initialize():\n",
    "    new_variables = set(tf.all_variables()) - ALREADY_INITIALIZED\n",
    "    get_session().run(tf.initialize_variables(new_variables))\n",
    "    ALREADY_INITIALIZED.update(new_variables)\n",
    "\n",
    "\n",
    "def eval(expr, feed_dict=None):\n",
    "    if feed_dict is None: feed_dict = {}\n",
    "    return get_session().run(expr, feed_dict=feed_dict)\n",
    "\n",
    "def set_value(v, val):\n",
    "    get_session().run(v.assign(val))\n",
    "\n",
    "def load_state(fname):\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(get_session(), fname)\n",
    "\n",
    "def save_state(fname):\n",
    "    os.makedirs(os.path.dirname(fname), exist_ok=True)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(get_session(), fname)\n",
    "\n",
    "# ================================================================\n",
    "# Model components\n",
    "# ================================================================\n",
    "\n",
    "\n",
    "def normc_initializer(std=1.0):\n",
    "    def _initializer(shape, dtype=None, partition_info=None): #pylint: disable=W0613\n",
    "        out = np.random.randn(*shape).astype(np.float32)\n",
    "        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        return tf.constant(out)\n",
    "    return _initializer\n",
    "\n",
    "def dense(x, size, name, weight_init=None, bias=True):\n",
    "    w = tf.get_variable(name + \"/w\", [x.get_shape()[1], size], initializer=weight_init)\n",
    "    ret = tf.matmul(x, w)\n",
    "    if bias:\n",
    "        b = tf.get_variable(name + \"/b\", [size], initializer=tf.zeros_initializer)\n",
    "        return ret + b\n",
    "    else:\n",
    "        return ret\n",
    "\n",
    "# ================================================================\n",
    "# Basic Stuff\n",
    "# ================================================================\n",
    "\n",
    "def function(inputs, outputs, updates=None, givens=None):\n",
    "    if isinstance(outputs, list):\n",
    "        return _Function(inputs, outputs, updates, givens=givens)\n",
    "    elif isinstance(outputs, dict):\n",
    "        f = _Function(inputs, outputs.values(), updates, givens=givens)\n",
    "        return lambda *inputs : dict(zip(outputs.keys(), f(*inputs)))\n",
    "    else:\n",
    "        f = _Function(inputs, [outputs], updates, givens=givens)\n",
    "        return lambda *inputs : f(*inputs)[0]\n",
    "\n",
    "class _Function(object):\n",
    "    def __init__(self, inputs, outputs, updates, givens, check_nan=False):\n",
    "        assert all(len(i.op.inputs)==0 for i in inputs), \"inputs should all be placeholders\"\n",
    "        self.inputs = inputs\n",
    "        updates = updates or []\n",
    "        self.update_group = tf.group(*updates)\n",
    "        self.outputs_update = list(outputs) + [self.update_group]\n",
    "        self.givens = {} if givens is None else givens\n",
    "        self.check_nan = check_nan\n",
    "    def __call__(self, *inputvals):\n",
    "        assert len(inputvals) == len(self.inputs)\n",
    "        feed_dict = dict(zip(self.inputs, inputvals))\n",
    "        feed_dict.update(self.givens)\n",
    "        results = get_session().run(self.outputs_update, feed_dict=feed_dict)[:-1]\n",
    "        if self.check_nan:\n",
    "            if any(np.isnan(r).any() for r in results):\n",
    "                raise RuntimeError(\"Nan detected\")\n",
    "        return results\n",
    "\n",
    "# ================================================================\n",
    "# Graph traversal\n",
    "# ================================================================\n",
    "\n",
    "VARIABLES = {}\n",
    "\n",
    "# ================================================================\n",
    "# Flat vectors\n",
    "# ================================================================\n",
    "\n",
    "def var_shape(x):\n",
    "    out = [k.value for k in x.get_shape()]\n",
    "    assert all(isinstance(a, int) for a in out), \\\n",
    "        \"shape function assumes that shape is fully known\"\n",
    "    return out\n",
    "\n",
    "def numel(x):\n",
    "    return intprod(var_shape(x))\n",
    "\n",
    "def intprod(x):\n",
    "    return int(np.prod(x))\n",
    "\n",
    "def flatgrad(loss, var_list):\n",
    "    grads = tf.gradients(loss, var_list)\n",
    "    return tf.concat(0, [tf.reshape(grad, [numel(v)])\n",
    "        for (v, grad) in zip(var_list, grads)])\n",
    "\n",
    "class SetFromFlat(object):\n",
    "    def __init__(self, var_list, dtype=tf.float32):\n",
    "        assigns = []\n",
    "        shapes = list(map(var_shape, var_list))\n",
    "        total_size = np.sum([intprod(shape) for shape in shapes])\n",
    "\n",
    "        self.theta = theta = tf.placeholder(dtype,[total_size])\n",
    "        start=0\n",
    "        assigns = []\n",
    "        for (shape,v) in zip(shapes,var_list):\n",
    "            size = intprod(shape)\n",
    "            assigns.append(tf.assign(v, tf.reshape(theta[start:start+size],shape)))\n",
    "            start+=size\n",
    "        assert start == total_size\n",
    "        self.op = tf.group(*assigns)\n",
    "    def __call__(self, theta):\n",
    "        get_session().run(self.op, feed_dict={self.theta:theta})\n",
    "\n",
    "class GetFlat(object):\n",
    "    def __init__(self, var_list):\n",
    "        self.op = tf.concat(0, [tf.reshape(v, [numel(v)]) for v in var_list])\n",
    "    def __call__(self):\n",
    "        return get_session().run(self.op)\n",
    "\n",
    "# ================================================================\n",
    "# Misc\n",
    "# ================================================================\n",
    "\n",
    "def scope_vars(scope, trainable_only):\n",
    "    \"\"\"\n",
    "    Get variables inside a scope\n",
    "    The scope can be specified as a string\n",
    "    \"\"\"\n",
    "    return tf.get_collection(\n",
    "        tf.GraphKeys.TRAINABLE_VARIABLES if trainable_only else tf.GraphKeys.VARIABLES,\n",
    "        scope=scope if isinstance(scope, str) else scope.name\n",
    "    )\n",
    "\n",
    "def in_session(f):\n",
    "    @functools.wraps(f)\n",
    "    def newfunc(*args, **kwargs):\n",
    "        with tf.Session():\n",
    "            f(*args, **kwargs)\n",
    "    return newfunc\n",
    "\n",
    "\n",
    "_PLACEHOLDER_CACHE = {} # name -> (placeholder, dtype, shape)\n",
    "def get_placeholder(name, dtype, shape):\n",
    "    print(\"calling get_placeholder\", name)\n",
    "    if name in _PLACEHOLDER_CACHE:\n",
    "        out, dtype1, shape1 = _PLACEHOLDER_CACHE[name]\n",
    "        assert dtype1==dtype and shape1==shape\n",
    "        return out\n",
    "    else:\n",
    "        out = tf.placeholder(dtype=dtype, shape=shape, name=name)\n",
    "        _PLACEHOLDER_CACHE[name] = (out,dtype,shape)\n",
    "        return out\n",
    "def get_placeholder_cached(name):\n",
    "    return _PLACEHOLDER_CACHE[name][0]\n",
    "\n",
    "def flattenallbut0(x):\n",
    "    return tf.reshape(x, [-1, intprod(x.get_shape().as_list()[1:])])\n",
    "\n",
    "def reset():\n",
    "    global _PLACEHOLDER_CACHE\n",
    "    global VARIABLES\n",
    "    _PLACEHOLDER_CACHE = {}\n",
    "    VARIABLES = {}\n",
    "    tf.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_session(single_threaded):\n",
    "    import tensorflow as tf\n",
    "    if not single_threaded:\n",
    "        return tf.InteractiveSession()\n",
    "    return tf.InteractiveSession(config=tf.ConfigProto(inter_op_parallelism_threads=1, intra_op_parallelism_threads=1))\n",
    "\n",
    "sess = make_session(single_threaded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#from . import tf_util as U\n",
    "\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.args, self.kwargs = args, kwargs\n",
    "        self.scope = self._initialize(*args, **kwargs)\n",
    "        self.all_variables = tf.get_collection(tf.GraphKeys.VARIABLES, self.scope.name)\n",
    "\n",
    "        self.trainable_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.scope.name)\n",
    "        #self.num_params = sum(int(np.prod(v.get_shape().as_list())) for v in self.trainable_variables)\n",
    "        self._setfromflat = SetFromFlat(self.trainable_variables)\n",
    "        self._getflat = GetFlat(self.trainable_variables)\n",
    "\n",
    "        #logger.info('Trainable variables ({} parameters)'.format(self.num_params))\n",
    "        for v in self.trainable_variables:\n",
    "            shp = v.get_shape().as_list()\n",
    "            logger.info('- {} shape:{} size:{}'.format(v.name, shp, np.prod(shp)))\n",
    "        logger.info('All variables')\n",
    "        for v in self.all_variables:\n",
    "            shp = v.get_shape().as_list()\n",
    "            logger.info('- {} shape:{} size:{}'.format(v.name, shp, np.prod(shp)))\n",
    "\n",
    "        placeholders = [tf.placeholder(v.value().dtype, v.get_shape().as_list()) for v in self.all_variables]\n",
    "        self.set_all_vars = function(\n",
    "            inputs=placeholders,\n",
    "            outputs=[],\n",
    "            updates=[tf.group(*[v.assign(p) for v, p in zip(self.all_variables, placeholders)])]\n",
    "        )\n",
    "\n",
    "    def _initialize(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save(self, filename):\n",
    "        assert filename.endswith('.h5')\n",
    "        with h5py.File(filename, 'w') as f:\n",
    "            for v in self.all_variables:\n",
    "                f[v.name] = v.eval()\n",
    "            # TODO: it would be nice to avoid pickle, but it's convenient to pass Python objects to _initialize\n",
    "            # (like Gym spaces or numpy arrays)\n",
    "            f.attrs['name'] = type(self).__name__\n",
    "            f.attrs['args_and_kwargs'] = np.void(pickle.dumps((self.args, self.kwargs), protocol=-1))\n",
    "\n",
    "    @classmethod\n",
    "    def Load(cls, filename, extra_kwargs=None):\n",
    "        with h5py.File(filename, 'r') as f:\n",
    "            args, kwargs = pickle.loads(f.attrs['args_and_kwargs'].tostring())\n",
    "            if extra_kwargs:\n",
    "                kwargs.update(extra_kwargs)\n",
    "            policy = cls(*args, **kwargs)\n",
    "            policy.set_all_vars(*[f[v.name][...] for v in policy.all_variables])\n",
    "        return policy\n",
    "\n",
    "    # === Rollouts/training ===\n",
    "\n",
    "    def rollout(self, env, *, render=False, timestep_limit=None, save_obs=False, random_stream=None):\n",
    "        \"\"\"\n",
    "        If random_stream is provided, the rollout will take noisy actions with noise drawn from that stream.\n",
    "        Otherwise, no action noise will be added.\n",
    "        \"\"\"\n",
    "        env_timestep_limit = GRID_SIZE - 2\n",
    "        timestep_limit = env_timestep_limit if timestep_limit is None else min(timestep_limit, env_timestep_limit)\n",
    "        rews = []\n",
    "        t = 0\n",
    "        if save_obs:\n",
    "            obs = []\n",
    "        ob = env.reset()\n",
    "        for _ in range(timestep_limit):\n",
    "            ac = self.act(ob[None], random_stream=random_stream)[0]\n",
    "            if save_obs:\n",
    "                obs.append(ob)\n",
    "            ob, rew, done, _ = env.step(ac)\n",
    "            rews.append(rew)\n",
    "            t += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        rews = np.array(rews, dtype=np.float32)\n",
    "        if save_obs:\n",
    "            return rews, t, np.array(obs)\n",
    "        return rews, t\n",
    "\n",
    "    def act(self, ob, random_stream=None):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def set_trainable_flat(self, x):\n",
    "        self._setfromflat(x)\n",
    "\n",
    "    def get_trainable_flat(self):\n",
    "        return self._getflat()\n",
    "\n",
    "    @property\n",
    "    def needs_ob_stat(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def set_ob_stat(self, ob_mean, ob_std):\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CatchPolicy(Policy):\n",
    "    def _initialize(self, ob_space, ac_space, nonlin_type, hidden_dims, connection_type):\n",
    "        self.ac_space = ac_space\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.connection_type = connection_type\n",
    "\n",
    "        assert len(ob_space.shape) == len(self.ac_space.shape) == 1\n",
    "        #assert np.all(np.isfinite(self.ac_space.low)) and np.all(np.isfinite(self.ac_space.high)), \\\n",
    "        #    'Action bounds required'\n",
    "\n",
    "        self.nonlin = {'tanh': tf.tanh, 'relu': tf.nn.relu,  'elu': tf.nn.elu}[nonlin_type]\n",
    "\n",
    "        with tf.variable_scope(type(self).__name__) as scope:\n",
    "\n",
    "            # Policy network\n",
    "            o = tf.placeholder(tf.float32, [None] + list(ob_space.shape))\n",
    "            a = self._make_net(o)\n",
    "            self._act = function([o], a)\n",
    "        return scope\n",
    "\n",
    "    def _make_net(self, o):\n",
    "        # Process observation\n",
    "        if self.connection_type == 'ff':\n",
    "            x = o\n",
    "            for ilayer, hd in enumerate(self.hidden_dims):\n",
    "                x = self.nonlin(dense(x, hd, 'l{}'.format(ilayer), normc_initializer(1.0)))\n",
    "        else:\n",
    "            raise NotImplementedError(self.connection_type)\n",
    "\n",
    "        # Map to action\n",
    "        \n",
    "        scores = dense(x, 3, 'out', normc_initializer(0.01))\n",
    "        scores_nab = tf.reshape(scores, [-1, 1, 3])\n",
    "        aidx_na =  tf.argmax(scores_nab, 2)  # 0 ... num_bins-1\n",
    "        a = tf.to_float(aidx_na)\n",
    "        print(a)\n",
    "       \n",
    "        return a\n",
    "\n",
    "    def act(self, ob, random_stream=None):\n",
    "        return self._act(ob)\n",
    "\n",
    "    @property\n",
    "    def needs_ob_stat(self):\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def needs_ref_batch(self):\n",
    "        return False\n",
    "\n",
    "    def initialize_from(self, filename, ob_stat=None):\n",
    "        \"\"\"\n",
    "        Initializes weights from another policy, which must have the same architecture (variable names),\n",
    "        but the weight arrays can be smaller than the current policy.\n",
    "        \"\"\"\n",
    "        with h5py.File(filename, 'r') as f:\n",
    "            f_var_names = []\n",
    "            f.visititems(lambda name, obj: f_var_names.append(name) if isinstance(obj, h5py.Dataset) else None)\n",
    "            assert set(v.name for v in self.all_variables) == set(f_var_names), 'Variable names do not match'\n",
    "\n",
    "            init_vals = []\n",
    "            for v in self.all_variables:\n",
    "                shp = v.get_shape().as_list()\n",
    "                f_shp = f[v.name].shape\n",
    "                assert len(shp) == len(f_shp) and all(a >= b for a, b in zip(shp, f_shp)), \\\n",
    "                    'This policy must have more weights than the policy to load'\n",
    "                init_val = v.eval()\n",
    "                # ob_mean and ob_std are initialized with nan, so set them manually\n",
    "                if 'ob_mean' in v.name:\n",
    "                    init_val[:] = 0\n",
    "                    init_mean = init_val\n",
    "                elif 'ob_std' in v.name:\n",
    "                    init_val[:] = 0.001\n",
    "                    init_std = init_val\n",
    "                # Fill in subarray from the loaded policy\n",
    "                init_val[tuple([np.s_[:s] for s in f_shp])] = f[v.name]\n",
    "                init_vals.append(init_val)\n",
    "            self.set_all_vars(*init_vals)\n",
    "\n",
    "        if ob_stat is not None:\n",
    "            ob_stat.set_from_init(init_mean, init_std, init_count=1e5)\n",
    "            \n",
    "GRID_SIZE = 10            \n",
    "            \n",
    "class catcher():\n",
    "    def __init__(self):\n",
    "        # self.ep = episode()\n",
    "        # self.ep.__next__()\n",
    "        self.observation_space = np.zeros((GRID_SIZE, GRID_SIZE)).ravel()\n",
    "        self.action_space = np.array([2])\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.ep.send(ac)\n",
    "\n",
    "    def reset(self):\n",
    "        self.ep = episode()\n",
    "        S, won, _, _ = self.ep.__next__()\n",
    "        return S\n",
    "    \n",
    "    \n",
    "def episode():\n",
    "    \"\"\"\n",
    "    Coroutine of episode.\n",
    "\n",
    "    Action has to be explicitly send to this coroutine.\n",
    "    \"\"\"\n",
    "    x, y, z = (\n",
    "        np.random.randint(0, GRID_SIZE),  # X of fruit\n",
    "        0,  # Y of dot\n",
    "        np.random.randint(1, GRID_SIZE - 1)  # X of basket\n",
    "    )\n",
    "    while True:\n",
    "        X = np.zeros((GRID_SIZE, GRID_SIZE))  # Reset grid\n",
    "        X[y, x] = 1.  # Draw fruit\n",
    "        bar = range(z - 1, z + 2)\n",
    "        X[-1, bar] = 1.  # Draw basket\n",
    "\n",
    "        # End of game is known when fruit is at penultimate line of grid.\n",
    "        # End represents either a win or a loss\n",
    "        end = int(y >= GRID_SIZE - 2)\n",
    "        rew = end\n",
    "        if end and x not in bar:\n",
    "            rew *= -10\n",
    "        \n",
    "        print(X)\n",
    "        action = yield X.ravel(), rew, end, None\n",
    "        \n",
    "        #assert action in [0, 1, 2]\n",
    "        if end:\n",
    "            break\n",
    "\n",
    "        z = np.min([np.max([z + int(action[0]) - 1, 1]), GRID_SIZE - 2])\n",
    "        y += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class catcherOffPolicy():\n",
    "    def __init__(self, memory_path):\n",
    "        self.memory = pickle.load(open('memoryList.pickle', 'rb'))\n",
    "        \n",
    "        self.observation_space = np.zeros((GRID_SIZE, GRID_SIZE)).ravel()\n",
    "        self.action_space = np.array([2])\n",
    "\n",
    "    def step(self, ac):\n",
    "        return self.ep.send(ac)\n",
    "\n",
    "    def reset(self):\n",
    "        self.ep = episode()\n",
    "        S, won, _, _ = self.ep.__next__()\n",
    "        return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-522ccea2fb7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mimg_saver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mimg_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'generator' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "def save_img():\n",
    "    if 'images' not in os.listdir('.'):\n",
    "        os.mkdir('images')\n",
    "    frame = 0\n",
    "    while True:\n",
    "        screen = (yield)\n",
    "        plt.imshow(screen[0], interpolation='none')\n",
    "        plt.savefig('images/%03i.png' % frame)\n",
    "        frame += 1\n",
    "    \n",
    "img_saver = save_img()\n",
    "img_saver.next()\n",
    "\n",
    "for _ in xrange(8):\n",
    "    g = episode()\n",
    "    S, _ = g.next()\n",
    "    img_saver.send(S)\n",
    "    try:\n",
    "        while True:\n",
    "            act = np.argmax(model.predict(S[np.newaxis]), axis=-1)[0] - 1\n",
    "            S, _ = g.send(act)\n",
    "            img_saver.send(S)\n",
    "\n",
    "    except StopIteration:\n",
    "        pass\n",
    "\n",
    "img_saver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = catcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"CatchPolicy/ToFloat:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "cp = CatchPolicy(env.observation_space ,env.action_space, 'tanh', [100, 100], 'ff')\n",
    "initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cp.initialize_from('/tmp/es_master_31842/snapshot_iter00900_rew1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  1.  1.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  1.  1.  0.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  1.  1.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  1.  1.  0.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  1.  1.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  1.  1.  0.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  1.  1.  0.  0.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  1.  1.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.], dtype=float32), 8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp.rollout(env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Required argument 'file' (pos 2) not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-93e269650080>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'memoryList.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Required argument 'file' (pos 2) not found"
     ]
    }
   ],
   "source": [
    "env = catcher()\n",
    "memory = []\n",
    "for i in range(10000):\n",
    "    episodList = []\n",
    "    Xprev = env.reset()\n",
    "    end = False\n",
    "    \n",
    "    while not end:\n",
    "        \n",
    "        action = np.random.randint(3)\n",
    "        X, rew, end, _ = env.step([action])\n",
    "        \n",
    "        episodList.append((Xprev, action, rew))\n",
    "        Xprev = X\n",
    "        \n",
    "        #if end:\n",
    "        #    print(rew)\n",
    "    memory.append(episodList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(memory,open('memoryList.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ob, rew, done, _ = env.step(ac)\n",
    "ob = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
